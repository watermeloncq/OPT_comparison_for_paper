{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91bc700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27160388-ae65-4c56-a9bf-a07b73518981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensorboard_data(log_path):\n",
    "    \"\"\"从TensorBoard日志文件中提取数据\"\"\"\n",
    "    # 初始化事件累加器\n",
    "    ea = event_accumulator.EventAccumulator(log_path,\n",
    "        size_guidance={  # 定义加载的最大事件数量\n",
    "            event_accumulator.SCALARS: 0,  # 加载所有标量数据\n",
    "        })\n",
    "    ea.Reload()  # 加载数据\n",
    "    \n",
    "    # 获取所有标量指标的标签\n",
    "    tags = ea.Tags()['scalars']\n",
    "    \n",
    "    # 存储所有指标的数据\n",
    "    dataframes = {}\n",
    "    for tag in tags:\n",
    "        # 获取该指标的所有事件\n",
    "        events = ea.Scalars(tag)\n",
    "        \n",
    "        # 转换为DataFrame\n",
    "        df = pd.DataFrame(events)\n",
    "        # 重命名列名使其更直观\n",
    "        df.columns = ['wall_time', 'step', 'value']\n",
    "        \n",
    "        dataframes[tag] = df\n",
    "    \n",
    "    return dataframes, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aea41b4-b652-4cc5-b66d-d059e98929dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_dataframe(dataframes):\n",
    "    \"\"\"将所有指标数据合并为一个DataFrame，保持原始标签名称\"\"\"\n",
    "    # 创建基础DataFrame，使用第一个指标的step作为索引\n",
    "    first_key = list(dataframes.keys())[0]\n",
    "    combined_df = pd.DataFrame(index=dataframes[first_key]['step'])\n",
    "    \n",
    "    # 遍历所有指标，将它们的值添加到合并的DataFrame中\n",
    "    for tag, df in dataframes.items():\n",
    "        # 保持原始标签名称\n",
    "        combined_df[tag] = df.set_index('step')['value']\n",
    "    \n",
    "    # 重置索引，将step变成一个普通列\n",
    "    combined_df.reset_index(inplace=True)\n",
    "    # 重命名'index'列为'Unnamed: 0'以匹配目标格式\n",
    "    combined_df.rename(columns={'index': 'Unnamed: 0'}, inplace=True)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cba5e40d-fbec-4386-ae7e-4e96d4be2806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available metrics:\n",
      "- rollout/ep_len_mean\n",
      "- rollout/ep_rew_mean\n",
      "- time/fps\n",
      "- train/approx_kl\n",
      "- train/clip_fraction\n",
      "- train/clip_range\n",
      "- train/entropy_loss\n",
      "- train/explained_variance\n",
      "- train/learning_rate\n",
      "- train/loss\n",
      "- train/policy_gradient_loss\n",
      "- train/std\n",
      "- train/value_loss\n",
      "\n",
      "文件已保存到当前目录: ./stocks_trained_log.csv\n",
      "\n",
      "数据预览:\n",
      "    step  rollout/ep_len_mean  rollout/ep_rew_mean  time/fps  train/approx_kl  \\\n",
      "0   2048                128.0            -0.350610     555.0              NaN   \n",
      "1   4096                128.0            -0.781948     532.0         0.006492   \n",
      "2   6144                128.0            -0.696733     527.0         0.009861   \n",
      "3   8192                128.0            -0.791840     523.0         0.011337   \n",
      "4  10240                128.0            -0.764007     524.0         0.014558   \n",
      "\n",
      "   train/clip_fraction  train/clip_range  train/entropy_loss  \\\n",
      "0                  NaN               NaN                 NaN   \n",
      "1             0.060156               0.2          -15.601842   \n",
      "2             0.133057               0.2          -15.582532   \n",
      "3             0.117432               0.2          -15.561103   \n",
      "4             0.163037               0.2          -15.548524   \n",
      "\n",
      "   train/explained_variance  train/learning_rate  train/loss  \\\n",
      "0                       NaN                  NaN         NaN   \n",
      "1                  0.198905               0.0003    0.018238   \n",
      "2                  0.423238               0.0003   -0.006505   \n",
      "3                  0.685784               0.0003   -0.009795   \n",
      "4                  0.441081               0.0003   -0.003138   \n",
      "\n",
      "   train/policy_gradient_loss  train/std  train/value_loss  \n",
      "0                         NaN        NaN               NaN  \n",
      "1                   -0.003370   0.998506          0.144749  \n",
      "2                   -0.010500   0.997042          0.045667  \n",
      "3                   -0.011888   0.994430          0.054383  \n",
      "4                   -0.015002   0.994775          0.028819  \n"
     ]
    }
   ],
   "source": [
    "# 读取tensorboard日志文件\n",
    "log_path = \"./data/stokcs/PPO_7/events.out.tfevents.1733476158.andy-X570-I.26362.0\"\n",
    "dataframes, tags = extract_tensorboard_data(log_path)\n",
    "\n",
    "# 打印所有可用的指标\n",
    "print(\"Available metrics:\")\n",
    "for tag in tags:\n",
    "    print(f\"- {tag}\")\n",
    "\n",
    "# 创建合并的DataFrame并保持原始标签名称\n",
    "combined_df = create_combined_dataframe(dataframes)\n",
    "\n",
    "# 保存为CSV文件到当前目录\n",
    "output_path = './stocks_trained_log.csv'\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "# 打印保存位置和数据预览\n",
    "print(f\"\\n文件已保存到当前目录: {output_path}\")\n",
    "print(\"\\n数据预览:\")\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ed9a3-fb37-4418-b474-b4753d946d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327302d-8d32-4d27-bb1a-eba21d2e02ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be3fe8-f37c-491d-856b-4512e121596d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe9305-fe11-481f-a3d5-97df2740a13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ca05b-ce5b-4aa2-b2cc-6970ff6fffb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0919c19-1e69-441a-bbd3-64ced3f65b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
